\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

\usepackage{changes}
\definechangesauthor[name={Anh}, color=red]{Anh}
\setremarkmarkup{(#2)}

% Basic Document Settings

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

% Math Environments

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{remark}
\newtheorem{comment}{Comment}[section]
\newtheorem{conjecture}{Conjecture}[section]

% Create Problem Sections

% \newcommand{\enterProblemHeader}[1]{
%     \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
%     \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
% }
%
% \newcommand{\exitProblemHeader}[1]{
%     \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
%     \stepcounter{#1}
%     \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
% }
%
% \setcounter{secnumdepth}{0}
% \newcounter{partCounter}
% \newcounter{homeworkProblemCounter}
% \setcounter{homeworkProblemCounter}{1}
% \nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}
%
% \newenvironment{homeworkProblem}{
%     \section{Problem \arabic{homeworkProblemCounter}}
%     \setcounter{partCounter}{1}
%     \enterProblemHeader{homeworkProblemCounter}
% }{
%     \exitProblemHeader{homeworkProblemCounter}
% }

% Homework Details

\newcommand{\hmwkTitle}{Homework\ \#3}
\newcommand{\hmwkDueDate}{April 9, 2014}
\newcommand{\hmwkClass}{Stat 732: Statistical Inference}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{Professor Tokdar}
\newcommand{\hmwkAuthorName}{Anh Le}

% Title Page

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 3:10pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

% \renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\pagebreak

<<'load_packages', message=FALSE, warning=FALSE>>=
library(metRology) #For scaled t-distribution
library(ggplot2)
library(reshape2)
library(plyr)
library(xtable)
library(gridExtra)
library(geoR) # For inverse chi square
library(mvtnorm) # For bivariate normal
@


\section{Problem 1}

  \subsection{Part A: Describe the posterior distribution of $(\mu_1, \mu_2, \sigma^2)$}

Consider the Gaussian model, $y_i \sim z_i^T\beta + \epsilon_i, \epsilon_i \sim^{IID} N(0, \sigma^2)$, in which:

\[ y = \left( \begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array} \right),
%
z = \left( \begin{array}{cc}
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1
\end{array} \right),
%
\beta = \begin{pmatrix}
\mu_1 \\
\mu_2
\end{pmatrix}
\]

<<'Q1a_input_data'>>=
n1 <- 12 ; n2 <- 7 ; n <- n1 + n2; p <- 2
y <- c(134, 146, 104, 119, 124, 161, 107, 83, 113, 129, 97, 123,
       70, 118, 101, 85, 107, 132, 94)
Z <- matrix(c( rep(1, n1), rep(0, n2), rep(0, n1), rep(1, n2) ), ncol=2)
@

<<'calculate_posterior_parameters', cache=TRUE>>=
posterior.question1 <- function(y, Z, a=0) {
  y[1:n1] <- y[1:n1] - a
  ZtZ <- t(Z)%*%Z
  beta_hat <- (solve(ZtZ)%*%t(Z)%*%y)
  s2 <- 1/(n-p) * sum((y - Z%*%beta_hat)^2)
  return(list(beta_hat=beta_hat, ZtZ=ZtZ, s2=s2))
}

beta_hat <- posterior.question1(y, Z)$beta_hat
ZtZ <- posterior.question1(y, Z)$ZtZ
s2 <- posterior.question1(y, Z)$s2
@


Thus, the posterior distribution is $(\mu_1, \mu_2, \sigma^2 | \text{data}) \sim N_p \chi^{-2}(\hat{\beta}_{LS}, Z'Z, n-p, s^2_{y|Z})$

So,
\[
\begin{split}
\frac{(n-p)s^2_{y|Z}}{\sigma^2} &\sim \chi^2(n-p) \\
\frac{\Sexpr{(n-p)*s2}}{\sigma^2} &\sim \chi^2(\Sexpr{n-p}) \\
\end{split},
\], and,
\[
\begin{split}
\beta | (\sigma^2 = s^2) &\sim N_p(\hat{\beta}_{LS}, s^2_{y|Z}(Z'Z)^{-1}) \\
\end{split}
\]

  \subsection{Part B: Describe the posterior distribution of $\mu_1 - \mu_2$}

We have that $(\beta, \sigma^2 | \text{data}) \sim N_p \chi^{-2}(\hat{\beta}_{LS}, Z'Z, n-p, s^2_{y|Z})$

We have $\mu_1 - \mu_2 = \begin{pmatrix} 1 \\ -1\end{pmatrix} \beta$, so
<<>>=
a <- c(1, -1)
@

\[ \begin{split}
\frac{a'\beta - a'\hat{\beta}_{LS}}{s\sqrt{a'(Z'Z)^{-1}a}} &\sim t_{n-p}(0,1)\\
\mu_1 - \mu_2 &\sim t_{n-p}(a'\hat{\beta}_{LS}, s^2 a'(Z'Z)^{-1}a)\\
\mu_1 - \mu_2 &\sim t_{\Sexpr{n-p}}(\Sexpr{t(a)%*%beta_hat}, \Sexpr{s2* t(a)%*%solve(ZtZ)%*%a}) \\
\end{split}
\]

  \subsection{Part C: Calculate the Bayes tail area probability for $H_0$}

$H_0: \mu_1 = \mu_2 \Rightarrow \mu_1 - \mu_2 = 0$

<<>>=
tail.prob <- 2 * pt.scaled(0, n-p, mean=t(a)%*%beta_hat, sd=sqrt(s2*t(a)%*%solve(ZtZ)%*%a))
tail.prob
@

So, the Bayes tail probability of $H_0 = \Sexpr{tail.prob}$

  \subsection{Part D}

\begin{align}
P(M_0 | data) &= \frac{P(data| M_0)P(M_0)}{P(data| M_0)P(M_0) + P(data| M_1)P(M_1)} \\
&= \frac{P(data| M_0)}{P(data| M_0) + P(data| M_1)} && \text{Since $P(M_0)=P(M_1)$} \\
&= \frac{BF_{01}}{BF_{01} + 1}
\end{align}

where

\begin{align}
P(data| M_0) &= \int \int f(data, \mu, \sigma^2 |M_0) \mathrm{d}\mu\mathrm{d}\sigma^2 \\
&= \int \int f(data| \mu, \sigma^2) \pi(\mu, \sigma^2|M_0) \mathrm{d}\mu\mathrm{d}\sigma^2 \\
P(data| M_1) &= \int\int f(data, \boldsymbol{\mu}, \sigma^2 |M_1)\mathrm{d}\boldsymbol{\mu}\mathrm{d}\sigma^2 \\
&=\int\int f(data|\boldsymbol{\mu}, \sigma^2) \pi(\boldsymbol{\mu}, \sigma^2|M_0) \mathrm{d}\boldsymbol{\mu}\mathrm{d}\sigma^2
\end{align}

See handwritten part on how to calculate $P(data| M_0)$ and $P(data| M_1)$.

Basic steps to calculate $P(data| M_0)$:
\begin{itemize}
  \item Integrate out $\mu$ by creating $\prod_i^n N(\bar{y}, \sigma^2)$
  \item Integrate out $\sigma^2$ by creating $\text{Inv-Gamma}(something, s_y^2 / 2)$
  \item We get:
  \[
  P(data|M_0) = \frac{\sqrt{n}}{(\sqrt{2\pi})^{n-1}} \frac{\Gamma(\frac{n-1}{2})}{(\frac{s^2_y}{2})^\frac{n-1}{2}}, s^2_y = \sum^n (y_i - \bar{y})^2
  \]
\end{itemize}

Basic steps to calculate $P(data| M_1)$:
\begin{itemize}
  \item Integrate out $\boldsymbol{\mu}$ by creating $N_n (\boldsymbol{\bar{y}}, \Sigma)$
  \item Integrate out $\Sigma$ by creating $\text{Inv-Wishart}(\Psi, something)$
  \item We get:
  \[
  P(data|M_1) = \frac{\sqrt{n_1n_2}}{(2\pi)^{n-2}} \frac{\Gamma(\frac{n-2}{2})}{(s^2_y)^{\frac{n-2}{2}}}, s^2_y = \sum^n (y_i - \bar{y})^2
  \]
\end{itemize}

<<'Q1D', cache=TRUE>>=
f_computeMarginal <- function(data, n1=12, n2=7) {
  # data is just a vector
  n <- length(data)
  s2data <- sum( (data - mean(data))^2 )
  P.data.M0 <- sqrt(n) / sqrt(2*pi)^(n-1) * gamma((n-1)/2) / (s2data/2)^((n-1)/2)
  P.data.M1 <- sqrt(n1*n2) / (sqrt(2*pi))^(n-2) * gamma((n-2)/2) / (s2data/2)^((n-2)/2)
  return(list(P.data.M0=P.data.M0, P.data.M1=P.data.M1))
}
res1d <- f_computeMarginal(y)
P.M0.data <- with(res1d, P.data.M0 / (P.data.M0 + P.data.M1) )
P.M0.data
@

So the posterior probability $P(H_0|data) = \Sexpr{P.M0.data}$. We reject the null with this analysis, whereas we fail to reject $H_0$ in part (c), in which the Bayes tail probability of $H_0 = \Sexpr{tail.prob}$.

  \subsection{Part E: Plot the point null posterior density and Bayes tail area for different values of $\mathrm{a}$}

<<'Q1E', fig.height=5>>=
a <- seq(0, 40, by=1)
post.parameters <- lapply(a, function(a) posterior.question1(y, Z, a))
posterior.null.density <- sapply(post.parameters, function(parameter) dt.scaled(0, n-p, mean=t(c(1,-1))%*%parameter$beta_hat, sd=sqrt(parameter$s2*t(c(1,-1))%*%solve(parameter$ZtZ)%*%c(1,-1))))

tail.prob.fun <- function(parameter) {
  mean <- t(c(1, -1))%*%parameter$beta_hat
  sd <- sqrt(parameter$s2 * t(c(1, -1)) %*% solve(parameter$ZtZ)%*%c(1, -1))
  return(2 * min(pt.scaled(0,n-p, mean, sd), 1 - pt.scaled(0, n-p, mean, sd)))
}
tail.prob.all <- sapply(post.parameters, tail.prob.fun)
plotData1e <- melt(data.frame(a=seq(0,40), post.null.density=posterior.null.density,
                       tail.prob=tail.prob.all), id.vars=c("a"))
ggplot(data=plotData1e) + geom_point(aes(x=a, y=value, shape=variable))
@

Comment: It seems like the test performs well, showing strong support for the null $H_0: \mu_1=\mu_2$ when there is indeed no difference between the sample.

  \subsection{Part F}

<<'Q1F', tidy=FALSE, fig.height=5, cache=TRUE>>=
y <- c(134, 146, 104, 119, 124, 161, 107, 83, 113, 129, 97, 123,
       70, 118, 101, 85, 107, 132, 94)
# Wrapping the formula to compute marginals in Question 1D into a function
f_computeMarginal <- function(data, n1=12, n2=7) {
  # data is just a vector
  n <- length(data)
  s2data <- sum( (data - mean(data))^2 )
  P.data.M0 <- sqrt(n) / sqrt(2*pi)^(n-1) * gamma((n-1)/2) / (s2data/2)^((n-1)/2)
  P.data.M1 <- sqrt(n1*n2) / (sqrt(2*pi))^(n-2) * gamma((n-2)/2) / (s2data/2)^((n-2)/2)
  return(list(P.data.M0=P.data.M0, P.data.M1=P.data.M1))
}

# The function to compute conditional BF given a training set
f_computeConditionalBayesFactor <- function(data, trainingIndex) {
  B.01 <- f_computeMarginal(data)$P.data.M0 / f_computeMarginal(data)$P.data.M1
  B.10.xl <- f_computeMarginal(data[trainingIndex])$P.data.M1 /
    f_computeMarginal(data[trainingIndex])$P.data.M0
  B.01.xl <- B.01 * B.10.xl
  return(B.01.xl)
}

# The function to compute intrinsic BF by averaging all conditional BFs
f_computeIntrinsicBayesFactor <- function(data, n1=12, n2=7) {
  #n1 and n2 is the number of obs in group
  data1 <- data[1:n1]
  data2 <- data[n1+1:n1+n2]
  minimalTrainingIndexSets <- expand.grid(1:n1, 1:n2, 1:n)
  conditionalBFs <- apply(minimalTrainingIndexSets, 1,
               function(row) f_computeConditionalBayesFactor(data, row))
  BF.AI <- mean(conditionalBFs, na.rm=T)
  BF.GI <- exp(mean(log(conditionalBFs), na.rm=T))
  return(list(BF.AI=BF.AI, BF.GI=BF.GI))
}

# Calculate the Intrinsic BF for all 40 shifted dataset
a <- seq(0, 40)
shiftedDatasets <- data.frame(t(sapply(a, function(a) c(y[1:n1]-a, y[(n1+1):(n1+n2)]))))
res1f <- data.frame(
  matrix(
    unlist(apply(shiftedDatasets, 1, f_computeIntrinsicBayesFactor)),
  ncol=2, byrow=T)
)
colnames(res1f) <- c("BF.AI", "BF.GI")

# Calculate M0 posterior density based on bayes factors
res1f$post.density.AI <- with(res1f, BF.AI / (BF.AI + 1))
res1f$post.density.GI <- with(res1f, BF.GI / (BF.GI + 1))

plotData1f <- melt(
  data.frame(a=seq(0, 40),
            post.null.density=posterior.null.density,
            tail.prob=tail.prob.all,
            res1f[,c("post.density.AI","post.density.GI")]),
id.vars=c("a"))

plot1f <- ggplot(data=plotData1f) + geom_point(aes(x=a, y=value, shape=variable))
plot1f
@

  \subsection{Part G}

<<'Q1G', fig.height=5, cache=TRUE>>=
pvalues <- apply(shiftedDatasets, 1,
      function(y) t.test(y[1:n1], y[(n1+1):(n1+n2)], var.equal=TRUE)$p.value)
BF.01.ttest <- ifelse(pvalues < 1/exp(1), -exp(1)*pvalues*log(pvalues), 1)
post.density.ttest <- BF.01.ttest / (BF.01.ttest + 1)

plotData1g <- melt(
  data.frame(a=seq(0, 40),
            post.null.density=posterior.null.density,
            tail.prob=tail.prob.all,
            res1f[,c("post.density.AI","post.density.GI")],
            post.density.ttest=post.density.ttest),
id.vars=c("a"))
plot1g <- ggplot(data=plotData1g) + geom_point(aes(x=a, y=value, shape=variable))
plot1g
@

We see that the posterior null density derived from the t-test p-value is indeed the lower bound.

\pagebreak

\section{Problem 2}

  \subsection{Part A: Find a simplified expression of the ratio $\xi(\mu) / \xi(\mu | X)$}

<<'Q2a', tidy=FALSE>>=
n <- 20 ; m0 <- 150; K0 <- 1; r0 <- 0.7; s02 <- 29.45^2; p0 <- 1
xbar <- 143.64; sx <- 41.44
mn <- (K0*m0 + n*xbar) / (K0+n)
Kn <- K0 + n
rn <- r0 + n
sn2 <- (r0*s02 + (n-1)*sx^2 + K0*n/(K0+n)*(xbar-m0)^2) / (r0+n)
@

We have: \[ \begin{split}
\text{Likelihood: } X_i &\sim^{IID} N(\mu, \sigma^2) \\
\text{Prior: } \pi(\mu, \sigma^2) &= N_{\chi^{-2}}(150, 1, 0.7, 29.45^2) \\
\text{Posterior: } \pi(\mu, \sigma^2 | X) &= N_{\chi^{-2}}(\Sexpr{c(mn, Kn, rn, sn2)}) \\
\end{split}
\]

So (based on property in handout): \[ \begin{split}
\xi(\mu) = \frac{\mu - \Sexpr{m0}}{\Sexpr{sqrt(s02*K0^(-1))}} &\sim t_{\Sexpr{r0}} \\
\xi(\mu|X) = \frac{\mu - \Sexpr{mn}}{\Sexpr{sqrt(sn2*Kn^(-1))}} &\sim t_{\Sexpr{rn}} \\
\end{split}\]

<<>>=
xi.mu <- function(mu) dt.scaled(mu, df=r0, mean=m0, sd=sqrt(s02*K0^(-1)))
xi.mu.x <- function(mu) dt.scaled(mu, df=rn, mean=mn, sd=sqrt(sn2*Kn^(-1)))

xi.mu.x(150) / xi.mu(150)
@
So $\frac{\xi(\mu | X)}{\xi(\mu)} = \Sexpr{xi.mu.x(150) / xi.mu(150)}$

  \subsection{Part B}

Consider $M_1$

\begin{align}
\pi(\sigma^2 | \mu) &= \frac{\pi(\sigma^2, \mu)}{\pi(\mu)} \\
&= \frac{N_{\chi^{-2}}(m, k, r, s^2)}{t_{r}(m, s\sqrt{k^{-1}})} \\
&= \frac{ const \cdot \sigma^{2 \cdot -\frac{r+p+2}{2}} \cdot \exp(- \frac{(\mu - m)^2 k + r s^2}{2\sigma^2})}{\frac{\Gamma(\frac{r+1}{2})}{\Gamma(\frac{r}{2})\sqrt{\pi r}s \sqrt{k^{-1}}} (1 + \frac{1}{r} \cdot (\frac{\mu-m}{s\sqrt{k^{-1}}})^{2})^{-\frac{r+1}{2}}} \\
\pi(\sigma^2 | \mu=150) &= const \cdot \frac{\exp(\frac{-rs^2}{2\sigma^2})}{\sigma^{2 \cdot \frac{r+p+2}{2}}} && (\text{Plug in $m=150$})
\end{align}

\added[id=Anh]{Notice that we could have gone straight to $\pi(\sigma^2 | \mu) \propto \pi(\sigma^2, \mu)$, recognizing that $\pi(\mu)$ does not involve $\sigma^2$ and therefore is just a constant.}

Consider $M_0$

\begin{align}
\sigma^2 &\sim \chi^{-2}(r_0, s_0^2) \\
\pi(\sigma^2) &= \frac{(\frac{r_0 s_0^2}{2})^{\frac{r_0}{2}}}{\Gamma(\frac{r_0}{2})} \cdot \frac{\exp(\frac{-r_0 s_0^2}{2\sigma^2})}{\sigma{2 \cdot (1 + \frac{r_0}{2})}} \\
&= const \cdot \frac{\exp(\frac{-r_0 s_0^2}{2\sigma^2})}{\sigma^{2 \cdot (1 + \frac{r_0}{2})}}
\end{align}

So we have:

\[\begin{cases}
r_0 s_0^2 = rs^2 = 0.7 \cdot 29.45^2 \\
1 + \frac{r_0}{2} = \frac{r+p+2}{2} = \frac{0.7 + 1 + 2}{2}
\end{cases} \Rightarrow
\begin{cases}
& r_0 = 1.7 \\
& s_0 = 18.897
\end{cases} \]

  \subsection{Part C}

\begin{align}
\mathrm{BF}_{01}(x) &= \frac{P(\text{data}|M_0)}{P(\text{data}|M_1)} \\
&= \frac{f_0(x)}{f_1(x)} \\
&= \frac{\frac{f_0(x|\sigma^{2*}) \cdot \pi_0(\sigma^{2*})}{\pi_0(\sigma^{2*}|x)}} {\frac{f_1(x|\mu^*, \sigma^{2*}) \cdot \pi_1(\mu^*, \sigma^{2*})}{\pi_1(\mu^*, \sigma^{2*}|x)}} \\
&= \frac {\frac{\pi_0(\sigma^{2*})}{\pi_0(\sigma^{2*}|x)}} {\frac{\pi_1(\mu^*, \sigma^{2*})}{\pi_1(\mu^*, \sigma^{2*}|x)}} && \text{Pick $\mu^*=150$, both likelihoods cancel out} \\
&= \frac{\frac{\chi^{-2}(r_0, s_0^2)}{\chi^{-2}(r_0 + n, \frac{r_0s_0^2 + \sum (x_i - \mu)^2}{r_0 + n})}}{\frac{N\chi^{-2}(m,k,r,s^2)}{N\chi^{-2}(m_n,k_n,r_n,s_n^2)}} \\
&= \frac{\frac{\chi^{-2}(r_0, s_0^2)}{\chi^{-2}(r_0 + n, \frac{r_0s_0^2 + \sum(x_i - \bar{x})^2 + \sum(\bar{x} - \mu)^2}{r_0 + n})}}{\frac{N\chi^{-2}(m,k,r,s^2)}{N\chi^{-2}(m_n,k_n,r_n,s_n^2)}} \\
&= \frac{N\chi^{-2}(m_n,k_n,r_n,s_n^2)}{\chi^{-2}(r_0 + n, \frac{r_0s_0^2 + \sum(x_i - \bar{x})^2 + \sum(\bar{x} - \mu)^2}{r_0 + n})} && \text{$r_0, s_0$ are chosen such that the two priors cancel out}
\end{align}

<<'Q2c', tidy=FALSE, cache=TRUE>>=
rm(list=ls())
n <- 20; xbar <- 143.64; sx <- 41.44 # data
r0 <- 1.7; s0 <- 18.897 # Model 0 prior
m <- 150; k <- 1; r <- 0.7; s <- 29.45 # Model 1 prior
# Model 1 posterior hyperparameters
mn <- (k*m + n*xbar) / (k + n)
kn <- k + n
rn <- r + n
sn <- sqrt( (r*s^2 + (n-1)*sx^2 + k*n*(xbar-m)^2/(k+n)) / (r+n) )

dnorminvchisq <- function(mu, sigma2, m, k, r, s, p = 1) {
  (r*s^2 / 2)^(r/2) / gamma(r/2) *
    (2*pi)^(-p/2) * (sigma2/k)^(-1/2) *
    sigma2^(-(r+p+2)/2) * exp( -((mu - m)^2 * k + r*s^2) / (2*sigma2) )
}

# Pick mu.star = 150 then the likelihood under both models cancel out
mu.star <- 150; sigma.star <- 41

BF01 <- exp(log(dnorminvchisq(mu.star, sigma.star^2, mn, kn, rn, sn)) -
  dinvchisq(sigma.star, r0 + n, (r0*s0^2 + sx^2 + n*(xbar-mu.star)^2) /
              (r0 + n), log=TRUE))
@

\pagebreak

\section{Problem 3: Variable Selection}

\[ \begin{split}
\frac{P(M_i|x)}{P(M_j|x)} &= \frac{\frac{P(x|M_i)P(M_i)}{f(x)}}{\frac{P(x|M_j)P(M_j)}{f(x)}} \\
&= \frac{P(x|M_i)}{P(x|M_j)} \times \frac{p_i}{p_j} \\
&= \mathrm{BF}_{ij} \times \frac{p_i}{p_j}
\end{split}
\]

\begin{conjecture}{Calculate posterior probability based on Bayes factor}\\
\[p_{\gamma}(x) = \frac{\mathrm{BF}_{\gamma 0} \times p_\gamma}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0} \times p_i} \]
\end{conjecture}

\begin{proof}
\begin{align}
p_{\gamma}(x) &= \frac{P(x|M_\gamma)P(M_\gamma)}{\sum_{i=0} P(x|M_i)P(M_i)} \\
&= \frac{\frac{P(x|M_\gamma)}{P(x|M_0)} P(M_\gamma)}{\sum_{i=0} \frac{P(x|M_i)}{P(x|M_0)} P(M_i)} \\
&= \frac{\mathrm{BF}_{\gamma 0} \times p_\gamma}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0} \times p_i}
\end{align}
\end{proof}

  \subsection{Part A: Independent Priors}

\begin{align}
p_{\gamma}(x) &= \frac{\mathrm{BF}_{\gamma 0} \times p_\gamma}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0} \times p_i}\\
&= \frac{\mathrm{BF}_{\gamma 0}}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0}}
\end{align}

<<results='asis'>>=
load('hw3.RData')
bf.all <- exp(lbf.all)
posteriorA <- bf.all / sum(bf.all)
wantedIndexA <- order(posteriorA, decreasing=TRUE)[1:10]
wantedModelA <- gam.all[wantedIndexA, ]
wantedPosteriorA <- posteriorA[wantedIndexA]
dfA <- data.frame(wantedPosteriorA, wantedModelA)
dfA$varList <- apply(dfA[2:15], 1, function(x) paste(names(x)[x], collapse=", "))
res3A <- dfA[, -2:-15]
colnames(res3A) <- c('Post ("ind")', 'varList')
xtable(res3A, digits=3, align="ccl")
@


  \subsection{Part B: Exchangeable priors}

\begin{align}
p_\gamma &= \int_0^1 p^{|\gamma|} (1-p)^{14 - |\gamma|} \mathrm{d}p \\
&= \mathrm{Beta}(|\gamma| + 1, 15 - |\gamma|)
\end{align}

\begin{align}
p_{\gamma}(x) &= \frac{\mathrm{BF}_{\gamma 0} \times p_\gamma}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0} \times p_i}\\
&= \frac{\mathrm{BF}_{\gamma 0} \times \mathrm{Beta}(|\gamma| + 1, 15 - |\gamma|)}{\sum_{i=0}^{\text{all}} \mathrm{BF}_{i0} \times \mathrm{Beta}(|i| + 1, 15 - |i|)}\\
\end{align}


<<results='asis'>>=
load('hw3.RData')
bf.all <- exp(lbf.all)
gam.size.all <- apply(gam.all, 1, sum)
posteriorB <- bf.all*beta(gam.size.all + 1, 15 - gam.size.all) /
  sum(bf.all*beta(gam.size.all + 1, 15 - gam.size.all))
wantedIndexB <- order(posteriorB, decreasing=TRUE)[1:10]
wantedModelB <- gam.all[wantedIndexB, ]
wantedPosteriorB <- posteriorB[wantedIndexB]
wantedPosteriorB_priorA <- posteriorA[wantedIndexB]
dfB <- data.frame(wantedPosteriorB, wantedPosteriorB_priorA, wantedModelB)
dfB$varList <- apply(dfB[3:16], 1, function(x) paste(names(x)[x], collapse=", "))
res3B <- dfB[, -3:-16]
colnames(res3B) <- c('Post ("exch")', 'Post ("ind")', 'varList')
xtable(res3B, digits=3, align="cccl")
@

  \subsection{Part C: Posterior inclusion probability of each var}

<<results='asis'>>=
prob_incl_A <- t(gam.all) %*% posteriorA
prob_incl_B <- t(gam.all) %*% posteriorB
res3C <- data.frame(prob_incl_A, prob_incl_B)
colnames(res3C) <- c("Prob Incl ('ind')", "Prob Incl ('exch')")
xtable(res3C, digits=3, align="ccc")
@

  \subsection{Part D: Comparisons between two choices of priors}

<<fig.height=5, fig.align='center'>>=
res3C$varName <- rownames(res3C)
ggplot(data=melt(res3C)) + geom_point(aes(x=varName, y=value, color=variable)) + theme(axis.text.x=element_text(angle=-45, hjust=0))
@

Both analyses have similar inclusion probability for all variables, except for a large difference in $\mathrm{tempS}$. Overall, the ``independent'' prior does lead to a higher inclusion probability across variables, however.

\pagebreak

\section{Problem 4}
  \subsection{Part A}

\begin{proof}
\begin{align}
\int_{\Theta} f^{*}(x^{*}|\theta)] \pi(\theta|x) \mathrm{d}\theta &= \int_{\Theta} f^{*}(x^{*}|\theta, x)] \pi(\theta|x) \mathrm{d}\theta \\
&= \int_{\Theta} f^{*}(x^{*}, \theta |x) \mathrm{d}\theta \\
&= f^{*}(x^{*}|x)
\end{align}
\end{proof}

  \subsection{Part B}

See handwritten part for details. The basic steps are:
\begin{align}
f(x) &= \int \int f(x, \mu, \sigma^2) \mathrm{d}\mu \mathrm{d}\sigma^2 \\
&= \int \int f(x|\mu, \sigma^2) f(\mu, \sigma^2) \mathrm{d}\mu \mathrm{d}\sigma^2 \\
&= \int \int N(\mu, \sigma^2) \times N_{1}\chi^{-2}(m, k, r, s^2) \mathrm{d}\mu \mathrm{d}\sigma^2
\end{align}

\begin{itemize}
 \item Integrate out $\mu$ first by creating the kernel of $\mu \sim N(\frac{x+km}{k+1}, \frac{\sigma}{\sqrt{k+1}})$
 \item Integrate out $\sigma^2$ by creating the kernel of $\sigma^2 \sim Inv-Gamma(\alpha, \beta)$, where \[
 \alpha = \frac{r+1}{2}; \\
 \beta = \frac{(\frac{x-m}{\sqrt{1+1/k}})^2 + rs^2}{2}
 \]
 \item The remaining constants and $x$ make up the scaled $t$ distribution
 \[
 \frac{X^* - m}{s \sqrt{1 + 1/k}} \sim t(r)
 \]
\end{itemize}

  \subsection{Part C}

\begin{align}
f(X^*|x) &= \int \int f(x^*|\mu, \sigma^2) f(\mu, \sigma^2|x) \mathrm{d}\mu \mathrm{d}\sigma^2 \\
&= \int \int N(\mu, \sigma^2) \times N\chi^{-2}(m_n, k_n, r_n, s_n^2) \mathrm{d}\mu \mathrm{d}\sigma^2
\end{align}

Then, similar to part 4b, we will have
\[
\frac{X^* - m_n}{s_n \sqrt{1 + \frac{1}{k_n}}} \sim t(r_n)
\]

<<'Q4c', tidy=FALSE>>=
rm(list=ls())
C <- c(92, 106, 123, 132, 151, 179, 203, 226, 248, 281, 308)
X <- log(C[2:length(C)] / C[1:length(C)-1])
n <- length(X) ; p <- 2
# There is only one covariate, which is a vector of 1's
mn <- mean(X) ; kn <- 10; rn <- n - p
sn <- sqrt(1/(n-p) * sum( (X - mean(X))^2 ) )

x.star <- rt.scaled(1000, df=rn, mean=mn, sd=sn*sqrt(1 + 1/kn))
x.star.quantile <- quantile(x.star, probs=c(0.025, 0.975))
c.next.quantile <- C[length(C)] * exp(x.star.quantile)
c.next.quantile
@

So the 95\% posterior predictive range for $X_{t=10+1} = \Sexpr{x.star.quantile}$ and for $C_{t=10+1} = \Sexpr{c.next.quantile}$

\pagebreak

\section{Problem 5}

\subsection{Part A: Show that $\theta = 5.66 \dot \lambda / (1-\lambda)$}

\begin{proof}
\begin{align}
\theta = \frac{P(s|b)}{P(s|w)} &= \frac{P(b|s)P(s)}{P(b)} \times \frac{P(w)}{P(w|s)P(s)} \\
&= \frac{P(b|s)}{P(w|s)} \times \frac{P(w)}{P(b)} \\
&= \frac{\lambda}{1-\lambda} \times \frac{P(w)}{P(b)} \\
&= 5.66 \times \frac{\lambda}{1-\lambda}
\end{align}
\end{proof}

\subsection{Part B: Show that $l(\lambda, \gamma, \kappa) = const + n_b \log(\gamma\lambda) + n_w \log(\kappa(1-\lambda)) + n_u \log((1-\gamma)\lambda + (1-\kappa)(1-\lambda))$}


\begin{proof}
\begin{align}
&f(\text{data}|\lambda, \gamma, \kappa) \\
&= P(\text{race rep, b}|s)^{n_b} \times P(\text{race rep, w}|s)^{n_w} \times (P(\text{race not rep, b}|s) + \text{race not rep, w}|s))^{n_u} \\
&= P(\text{rep}|b,s)P(b|s)^{n_b} \times P(\text{rep}|w,s)P(w|s)^{n_w} \times (P(\text{not rep}|b,s)P(b|s) + \text{not rep}|w,s)P(w|s))^{n_u} \\
&\log f(\text{data}|\lambda, \gamma, \kappa) \\
&= \text{const.} + n_b \log(\gamma\lambda) + n_w \log(\kappa(1-\lambda)) + n_u \log((1-\gamma)\lambda + (1-\kappa)(1-\lambda))
\end{align}
\end{proof}

\subsection{Part C: }

\begin{align}
l_r(\lambda, \kappa) &= l(\lambda, \gamma, \kappa) && (\text{with substitution $\gamma = \frac{r\kappa}{r\kappa + 1 - \kappa}$)} \\
\log \pi(\lambda, \kappa) &= \mathrm{Beta}(1.2, 6.8) \times \mathrm{Unif}(0,1)
\end{align}

With those two equations, we can plug in and write down our objective function (to be minimized with \textit{nlm})
\[
g(\lambda, \kappa) = - l_r(\lambda, \kappa) - \log \pi(\lambda, \kappa)
\]

<<'Question_5',tidy=FALSE, results='asis', warning=FALSE, cache=TRUE>>=
rm(list=ls())
nb <- 127; nw <- 148; nu <- 617
r.values <- c(1, 2, 3)

ll.lamb.kap <- function(lamb, kap, r) {
  gam <- (r * kap) / (r * kap + 1 - kap)
  return(nb*log(gam*lamb) + nw*log(kap*(1-lamb)) +
           nu*log((1-gam)*lamb + (1-kap)*(1-lamb)))
}

l.prior.lamb.kap <- function(lamb, kap) {
  dbeta(lamb, 1.2, 6.8, log=TRUE) + dunif(kap, 0, 1, log=TRUE)
}
g.obj.fun <- function(argmin, r) {
  lamb <- argmin[1]
  kap <- argmin[2]
  return(-ll.lamb.kap(lamb, kap, r) - l.prior.lamb.kap(lamb, kap))
}

#Starting values of lamb and kap are prior means
f_answerProb5 <- function(r) {
  argmin <- nlm(g.obj.fun, c(0.15, 0.5), hessian=TRUE, r)

  post.sample <- data.frame(rmvnorm(n=10000,
                            mean=argmin$estimate, sigma=solve(argmin$hessian)))
  colnames(post.sample) <- c("lamb", "kap")
  post.sample$theta <- 5.66 * post.sample$lamb / (1 - post.sample$lamb)

  lamb.95CI <- quantile(post.sample$lamb, prob=c(0.025, 0.975))
  theta.95CI <- quantile(post.sample$theta, prob=c(0.025, 0.975))
  prob.theta.le1 <- sum(post.sample$theta <= 1)

  return(list(lamb.95CI=lamb.95CI, theta.95CI=theta.95CI,
              prob.theta.le1=prob.theta.le1))
}

res5 <- lapply(r.values, f_answerProb5)
table5 <- data.frame(matrix(unlist(res5), ncol=5, byrow=T))
colnames(table5) <- c("$\\lambda 2.5\\%$", "$\\lambda 97.5\\%$",
  "$\\theta 2.5\\%$", "$\\theta 97.5\\%$", "$Pr(\\theta \\leq 1)$")
print(xtable(table5), sanitize.text.function=function(x){x})
@

\subsection{Part D: Is there evidence for racial profiling?}

Even when $r=3$, i.e. the odds of reporting when black is 3 times the odds of reporting when white, we still see strong evidence that $\theta >= 1$, i.e. blacks are stopped more frequently. Thus, even if we give the troopers strong benefit of the doubt, there is still evidence for racial profiling.

\end{document}